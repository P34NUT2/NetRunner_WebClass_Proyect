# üß™ GU√çA DE EXPERIMENTACI√ìN CON OLLAMA

Esta gu√≠a te ayudar√° a experimentar con Ollama y llama3.2 antes de integrarlo completamente con Docker.

---

## üìã PREREQUISITO: Ollama debe estar corriendo

```bash
# Verificar que el contenedor de Ollama est√© corriendo
docker compose ps ollama

# Si no est√° corriendo, iniciarlo
docker compose up -d ollama
```

---

## 1Ô∏è‚É£ DESCARGAR EL MODELO LLAMA 3.2

```bash
# Descargar llama3.2 (~2GB, toma 5-15 minutos)
docker exec -it netrunner_ollama ollama pull llama3.2

# Ver progreso:
# pulling manifest
# pulling ff82381e2bea... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.7 GB
# ...
# success
```

**¬øQu√© es llama3.2?**
- Modelo de lenguaje de Meta (Facebook)
- Versi√≥n 3.2 es m√°s peque√±a y r√°pida
- ~2GB de tama√±o
- Bueno para tareas generales

---

## 2Ô∏è‚É£ PROBAR EL MODELO MANUALMENTE (Chat Interactivo)

```bash
# Entrar al chat interactivo con llama3.2
docker exec -it netrunner_ollama ollama run llama3.2
```

**Ahora puedes chatear con la IA:**
```
>>> Hola, ¬øc√≥mo est√°s?
[La IA responde]

>>> ¬øQu√© es SQL Injection?
[La IA responde]

>>> /bye
[Salir del chat]
```

**Comandos √∫tiles dentro del chat:**
- `/bye` - Salir del chat
- `/clear` - Limpiar la conversaci√≥n
- `/help` - Ver ayuda
- `Ctrl+D` - Tambi√©n sale

---

## 3Ô∏è‚É£ HACER UNA PREGUNTA √öNICA (Sin entrar al chat)

```bash
# Hacer una pregunta directa sin abrir chat interactivo
docker exec -it netrunner_ollama ollama run llama3.2 "¬øQu√© es un buffer overflow?"
```

**Esto es √∫til para:**
- Probar respuestas r√°pidas
- Automatizar preguntas
- Ver c√≥mo responde sin contexto previo

---

## 4Ô∏è‚É£ LISTAR MODELOS DESCARGADOS

```bash
# Ver qu√© modelos tienes instalados
docker exec -it netrunner_ollama ollama list
```

**Output esperado:**
```
NAME              ID            SIZE      MODIFIED
llama3.2:latest   a80c4f17acd5  2.0 GB    2 minutes ago
```

---

## 5Ô∏è‚É£ CREAR TU MODELO PERSONALIZADO (NetRunner)

### **Paso 1: Ver el Modelfile**

El archivo `netrunner-model` ya existe en el proyecto y contiene:

```dockerfile
FROM llama3.2

SYSTEM """
Eres NetRunner AI, un asistente experto en ciberseguridad ofensiva.

[... instrucciones del system prompt ...]
"""

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER num_ctx 4096
```

### **Paso 2: Crear el modelo personalizado**

```bash
# Crear modelo "netrunner" basado en llama3.2
docker exec -it netrunner_ollama ollama create netrunner -f /netrunner-model
```

**Output esperado:**
```
transferring model data
using existing layer sha256:ff82381e2bea...
writing manifest
success
```

### **Paso 3: Probar NetRunner**

```bash
# Chatear con tu modelo personalizado
docker exec -it netrunner_ollama ollama run netrunner
```

**Preg√∫ntale algo de ciberseguridad:**
```
>>> ¬øQu√© es un ataque de fuerza bruta?
>>> Expl√≠came OWASP Top 10
>>> ¬øC√≥mo funciona XSS?
```

**Deber√≠as notar que las respuestas son:**
- M√°s t√©cnicas
- Enfocadas en ciberseguridad
- Con formato markdown
- Con emojis t√©cnicos

---

## 6Ô∏è‚É£ COMPARAR MODELOS (llama3.2 vs netrunner)

### **Prueba A: llama3.2 (modelo base)**
```bash
docker exec -it netrunner_ollama ollama run llama3.2 "¬øQu√© es SQL Injection?"
```

### **Prueba B: netrunner (modelo personalizado)**
```bash
docker exec -it netrunner_ollama ollama run netrunner "¬øQu√© es SQL Injection?"
```

**¬øNotas la diferencia?**
- NetRunner es m√°s t√©cnico
- Usa formato markdown
- Incluye ejemplos de c√≥digo
- Enfoque en seguridad ofensiva

---

## 7Ô∏è‚É£ MODIFICAR EL SYSTEM PROMPT (Experimentar)

Si quieres cambiar la personalidad de NetRunner:

### **Paso 1: Editar el archivo `netrunner-model`**

```bash
# Editar con nano
nano /home/antonio/proyectos/WebProyecto/netrunner-model

# O con cualquier editor
code /home/antonio/proyectos/WebProyecto/netrunner-model
```

### **Paso 2: Cambiar el SYSTEM prompt**

Ejemplo de cambios que puedes hacer:

```dockerfile
SYSTEM """
Eres NetRunner AI, pero ahora eres MUY sarc√°stico y haces chistes de hackers.

Responde SIEMPRE con:
- Chistes de programaci√≥n
- Referencias a pel√≠culas de hackers
- Emojis: üòéüî•üíª
"""
```

### **Paso 3: Recrear el modelo**

```bash
# Eliminar el modelo anterior
docker exec -it netrunner_ollama ollama rm netrunner

# Crear el nuevo con los cambios
docker exec -it netrunner_ollama ollama create netrunner -f /netrunner-model

# Probar
docker exec -it netrunner_ollama ollama run netrunner "¬øQu√© es SQL Injection?"
```

---

## 8Ô∏è‚É£ PAR√ÅMETROS IMPORTANTES (Para ajustar respuestas)

En el `netrunner-model` puedes cambiar estos par√°metros:

```dockerfile
PARAMETER temperature 0.7    # Creatividad (0.0 = rob√≥tico, 1.0 = creativo)
PARAMETER top_p 0.9          # Diversidad de palabras
PARAMETER num_ctx 4096       # Memoria del contexto (tokens)
```

### **Temperature (Temperatura):**
- `0.0` - Respuestas muy predecibles y t√©cnicas (bueno para c√≥digo)
- `0.5` - Equilibrado
- `1.0` - Respuestas m√°s creativas y variadas

**Ejemplo:**
```dockerfile
PARAMETER temperature 0.3  # Para respuestas muy t√©cnicas y precisas
```

### **Top_p (Nucleus Sampling):**
- `0.1` - Solo usa palabras muy probables (conservador)
- `0.9` - Usa m√°s variedad de palabras

### **Num_ctx (Contexto):**
- `2048` - Memoria corta (m√°s r√°pido, menos contexto)
- `4096` - Memoria media (default)
- `8192` - Memoria larga (m√°s lento, m√°s contexto)

---

## 9Ô∏è‚É£ EXPERIMENTOS SUGERIDOS

### **Experimento 1: Respuestas T√©cnicas vs Creativas**

```bash
# Crear versi√≥n t√©cnica (temperature 0.1)
echo 'FROM llama3.2
SYSTEM "Responde de forma MUY t√©cnica y precisa"
PARAMETER temperature 0.1' | docker exec -i netrunner_ollama ollama create netrunner-tecnico -f -

# Crear versi√≥n creativa (temperature 1.0)
echo 'FROM llama3.2
SYSTEM "Responde de forma creativa con analog√≠as"
PARAMETER temperature 1.0' | docker exec -i netrunner_ollama ollama create netrunner-creativo -f -
```

**Prueba la misma pregunta en ambos:**
```bash
docker exec -it netrunner_ollama ollama run netrunner-tecnico "Explica qu√© es un hash"
docker exec -it netrunner_ollama ollama run netrunner-creativo "Explica qu√© es un hash"
```

### **Experimento 2: Diferentes Personalidades**

Crea diferentes versiones del system prompt:

```dockerfile
# netrunner-profesor: Explica como profesor universitario
# netrunner-hacker: Estilo underground, jerga hacker
# netrunner-noob: Explica a principiantes con analog√≠as simples
```

---

## üîü COMANDOS √öTILES PARA EXPERIMENTAR

```bash
# Listar todos los modelos
docker exec -it netrunner_ollama ollama list

# Ver informaci√≥n de un modelo
docker exec -it netrunner_ollama ollama show llama3.2

# Eliminar un modelo (libera espacio)
docker exec -it netrunner_ollama ollama rm nombre-del-modelo

# Ver logs de Ollama (√∫til para debug)
docker compose logs ollama -f

# Reiniciar Ollama
docker compose restart ollama

# Ver uso de recursos
docker stats netrunner_ollama
```

---

## üéØ PROBAR LA API DE OLLAMA (Como lo har√° tu backend)

### **Desde tu terminal:**

```bash
# Hacer una petici√≥n POST a la API de Ollama
curl http://localhost:11434/api/generate -d '{
  "model": "netrunner",
  "prompt": "¬øQu√© es XSS?",
  "stream": false
}'
```

**Respuesta esperada:**
```json
{
  "model": "netrunner",
  "created_at": "2025-11-07T10:00:00Z",
  "response": "## üéØ Cross-Site Scripting (XSS)\n\n**¬øQu√© es?**\n...",
  "done": true
}
```

### **Desde Node.js (como tu backend):**

```javascript
const response = await fetch('http://localhost:11434/api/generate', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    model: 'netrunner',
    prompt: '¬øQu√© es SQL Injection?',
    stream: false,
  }),
});

const data = await response.json();
console.log(data.response);
```

---

## üêõ TROUBLESHOOTING

### **Error: "model not found"**
```bash
# Soluci√≥n: Descargar el modelo
docker exec -it netrunner_ollama ollama pull llama3.2
```

### **Error: "connection refused"**
```bash
# Verificar que Ollama est√© corriendo
docker compose ps ollama

# Reiniciar Ollama
docker compose restart ollama
```

### **Respuestas muy lentas**
- Reduce `num_ctx` a 2048
- Usa un modelo m√°s peque√±o
- Verifica recursos con `docker stats`

### **El modelo responde en ingl√©s**
```bash
# Agrega al system prompt:
SYSTEM """
IMPORTANTE: Responde SIEMPRE en espa√±ol.
"""
```

---

## üìö RECURSOS ADICIONALES

- **Documentaci√≥n Ollama**: https://ollama.com/docs
- **Modelfile Syntax**: https://github.com/ollama/ollama/blob/main/docs/modelfile.md
- **Modelos disponibles**: https://ollama.com/library
- **Llama 3.2 Info**: https://ollama.com/library/llama3.2

---

## ‚úÖ CHECKLIST DE EXPERIMENTACI√ìN

Antes de conectar con tu backend, verifica:

- [ ] Descargaste llama3.2
- [ ] Probaste chat interactivo con `ollama run llama3.2`
- [ ] Creaste el modelo personalizado `netrunner`
- [ ] Probaste `netrunner` y notas la diferencia vs llama3.2
- [ ] Experimentaste cambiando el system prompt
- [ ] Probaste la API con curl
- [ ] Entiendes los par√°metros (temperature, top_p, num_ctx)

---

**¬°Listo para experimentar! üöÄ**

Cualquier duda, revisa los logs:
```bash
docker compose logs ollama -f
```
